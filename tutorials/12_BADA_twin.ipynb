{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc673e527062b8c6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# TUTORIAL: Real-time bias-aware data assimilation\n",
    "<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>\n",
    "\n",
    "* [What is bias-aware data assimilation?](#BADA)\n",
    "    * [The source of bias: model accuracy vs computational cost](#source)\n",
    "    * [Algorithmic view on bias-aware data assimilation](#algo)\n",
    "* [What is bias-aware data assimilation, a bit more technically?](#tech)\n",
    "    * [Aleatoric and epistemic uncertainties](#UQ)\n",
    "    * [Augmented state-space formulation](#augment)\n",
    "    * [Model bias in a stochastic ensemble framework](#bias-stoch)\n",
    "    * [How do we estimate the bias in time?](#evolution)\n",
    "* [The regularized ensemble Kalman filter](#rEnKF)\n",
    "    * [Test case: van der Pol model with manually added bias](#test)\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab88643302cdcb9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<a class=\"anchor\" id=\"BADA\"></a>\n",
    "# 1. What is bias-aware data assimilation? \n",
    "Real-time data assimilation combines quick estimates of a physical state from an numerical model with experimental data.  In reality, numerical models are not a perfect representation of the physical quantity which we aim to predict because they typically rely on assumptions and simplifications for computational, i.e, they have an inherent model bias.\n",
    "\n",
    "<br /> \n",
    "\n",
    "<a class=\"anchor\" id=\"source\"></a>\n",
    "## 1.1. The source of bias: model accuracy vs computational cost \n",
    "\n",
    "The choice of physical model plays a key role in the assimilation. Numerical models are mathematical representations of the processes that govern the behaviour of a physical system. For example, in weather forecasting, models simulate how the atmosphere evolves over time based on physical equations describing fluid dynamics, thermodynamics, etc. The more physical information we add to the model, the higher the accuracy of the model estimates. However, the computational requirements increase with the model complexity.\n",
    "\n",
    "To perform real-time data assimilation we need quick estimates of the physical system. Thus, performing real-time high-fidelity modelling is not plausible in realistic scenarios. In order to apply real-time data assimilation to low-fidelity models, we must provide an estimate of the bias in the numerical model, i.e., the model error that we introduce when simplifying the physical equations.  \n",
    "\n",
    "\n",
    "<a><img src=\"https://magrilab.ae.ic.ac.uk/uploads/9/6/2/1/96210914//pyramid_1.png\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce318d78022dc50",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<a class=\"anchor\" id=\"algo\"></a>\n",
    "## 1.2. Algorithmic view on bias-aware data assimilation \n",
    "\n",
    "Algorithmically, we can summarize the bias-aware data assimilation process as:\n",
    "1. <em>Forecast</em>: propagate the imperfect numerical model in time to provide a <strong><font color=\"#e0915c\">biased forecast</font></strong> when <strong><font color=\"#24678d\">observation</font></strong> data become available.\n",
    "2. <em>Bias correction</em>: provide an estimate the bias, and project the <strong><font color=\"#e0915c\">biased forecast</font></strong> into an <strong><font color=\"#a561d0\">unbiased forecast</font></strong><font color=\"#2a2a2a\"></font>. \n",
    "3. <em>Assimilation</em>: combine optimally the <strong><font color=\"#a561d0\">unbiased forecast</font></strong> with the <strong><font color=\"#24678d\">observations</font></strong>. The direct assimilation results in an <strong><font color=\"#91e05c\">unbiased analysis</font></strong>, and the <strong><font color=\"#76cae9\">biased analysis</font></strong> is an indirect by-product of the assimilation.\n",
    "4. <em>Update</em>: the <strong><font color=\"#76cae9\">biased analysis</font></strong> is the initial condition for the new forecast step.\n",
    "\n",
    "<a><img src=\"https://magrilab.ae.ic.ac.uk/uploads/9/6/2/1/96210914//wbada-4_1.gif   \"></a> \n",
    "\n",
    "<br /> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d203305de11c50",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<a class=\"anchor\" id=\"tech\"></a>\n",
    "# 2. What is bias-aware data assimilation, a bit more technically? \n",
    "\n",
    "In this section, we revisit the formulation of the stochastic framework introduced in tutorial ***10_DA_intro_twin_VdP.ipynb*** and we extend it to account for model biases. \n",
    "\n",
    "<br />\n",
    "<a class=\"anchor\" id=\"UQ\"></a>\n",
    "## 2.1. Aleatoric and epistemic uncertainties  \n",
    " \n",
    "\n",
    "First, we discuss the statistical hypotheses on the aleatoric uncertainties. The aleatoric uncertainties contaminate the state and parameters as \n",
    "$$\n",
    "\\begin{equation}\n",
    "     \\boldsymbol{\\phi}  +  \\boldsymbol{\\epsilon}_\\phi =  \\boldsymbol{\\phi}^\\dagger, \\quad\n",
    "     \\boldsymbol{\\alpha} +  \\boldsymbol{\\epsilon}_\\alpha =  \\boldsymbol{\\alpha}^\\dagger,\n",
    "\\end{equation}\n",
    "$$\n",
    "where $\\dagger$ indicates the true quantity (which is unknown). The aleatoric uncertainties are modelled as Gaussian distributions   \n",
    "$$\n",
    "\\begin{equation}\n",
    "     \\boldsymbol{\\epsilon}_\\phi \\sim \\mathcal{N}( \\mathbf{0},  \\mathbf{C}_{\\phi\\phi} ), \\quad  \\boldsymbol{\\epsilon}_\\alpha \\sim \\mathcal{N}( \\mathbf{0},  \\mathbf{C}_{\\alpha\\alpha} ),\n",
    "\\end{equation}\n",
    "$$\n",
    "where $\\mathcal{N}( \\mathbf{0},  \\mathbf{C})$ is a normal distribution with zero mean and covariance $ \\mathbf{C}$. Second, we discuss model biases, which are epistemic uncertainties. The model bias is defined as the expected difference between the true observable and the model observable, i.e., \n",
    "$$\n",
    "\\begin{equation} \n",
    "   \\mathbf{b}   =  \\mathbf{d}^{\\dagger} -\\mathbb{E}( \\mathbf{q}). \n",
    "\\end{equation}\n",
    "$$\n",
    "Hence, the bias-corrected model observable is\n",
    "$$\n",
    "\\begin{equation} \n",
    "     \\mathbf{y} =  \\mathbf{q} +  \\mathbf{b} +  \\boldsymbol{\\epsilon}_q, \n",
    "\\end{equation}\n",
    "$$\n",
    "where $  \\boldsymbol{\\epsilon}_q \\sim \\mathcal{N}( \\mathbf{0},  \\mathbf{C}_{qq})$. (If the model is unbiased, $ \\mathbf{d}^{\\dagger} =\\mathbb{E}( \\mathbf{q})$.) \n",
    "\n",
    "\n",
    "The final model equations, which define the first source of information on the system, are\n",
    "$$\n",
    "\\begin{align} \n",
    "\\left\\{\n",
    "\\begin{array}{rcl}\n",
    "      \\mathrm{d} \\boldsymbol{\\phi}&=& \\mathcal{F}\\left( \\boldsymbol{\\phi+ \\boldsymbol{\\epsilon}_\\phi, \\alpha+ \\boldsymbol{\\epsilon}_\\alpha} \\right)  \\mathrm{d} t \\\\\n",
    "       \\mathbf{y} &=& \\mathcal{M}( \\boldsymbol{\\theta},  \\boldsymbol{\\phi}) +  \\mathbf{b}+  \\boldsymbol{\\epsilon}_q\n",
    "\\end{array}\n",
    "\\right.\n",
    "\\end{align}\n",
    "$$\n",
    "This set of equations is not closed because we need a model for the model bias. To infer it, we analyse  the residuals between the forecast and the observations, which are also known as <em>innovations<em/>\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\mathbf{i} =  \\mathbf{d} -  \\mathbf{q}. \n",
    "\\end{equation}\n",
    "$$\n",
    "As discussed in tutorial ***10_DA_intro_twin_VdP.ipynb***, the experimental data are affected by aleatoric noise such that $\\mathbf{d} + \\boldsymbol{\\epsilon}_d =  \\mathbf{d}^{\\dagger}$, where  $\\boldsymbol{\\epsilon}_d \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{C}_{dd})$ and  $\\mathbf{C}_{dd}$ is a diagonal matrix with diagonal $\\boldsymbol{\\epsilon}_d$. With this, the expected value of the innovation is \n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\mathbb{E}( \\mathbf{i}) =  \\mathbf{b}, \n",
    "\\end{equation}\n",
    "$$\n",
    "i.e., the expected innovation is the model bias. \n",
    "\n",
    "\n",
    "<br />\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650f29a5b0d185aa",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 2.2. Augmented state-space formulation <a class=\"anchor\" id=\"augment\"></a>\n",
    "The augmented form of the state-space formulation of the model, accounting for th emodel bias, yields \n",
    "$$\n",
    "\\begin{align}\n",
    "\\left\\{\n",
    "\\begin{array}{rcl}\n",
    "      \\mathrm{d}\\begin{bmatrix}\n",
    "      \\boldsymbol{\\phi}\\\\\n",
    "      \\boldsymbol{\\alpha}\\\\\n",
    "      \\mathbf{q}\n",
    " \\end{bmatrix} &=& \n",
    " \\begin{bmatrix}\n",
    "     \\mathcal{F}( \\boldsymbol{\\phi}+ \\boldsymbol{\\epsilon}_\\phi, \\boldsymbol{\\alpha}+ \\boldsymbol{\\epsilon}_\\alpha)\\\\\n",
    "      \\mathbf{0}_{N_\\alpha}\\\\\n",
    "      \\mathbf{0}_{N_q}\n",
    " \\end{bmatrix} \n",
    "{ \\mathrm{d} t}  \\\\\n",
    " \\mathbf{y} &=&  \\mathbf{q} +  \\mathbf{b}  +  \\boldsymbol{\\epsilon}_{q}\n",
    "\\end{array}\n",
    "\\right.\n",
    "\\quad\\leftrightarrow  \\quad\n",
    "\\left\\{\n",
    "\\begin{array}{rcl}\n",
    " \\mathrm{d} \\boldsymbol{\\psi} &=&  \\mathbf{F}\\left( \\boldsymbol{\\psi} + \\boldsymbol{\\epsilon}_\\psi\\right){ \\mathrm{d} t}  \\\\\n",
    " \\mathbf{y} &=&  \\mathbf{M} \\boldsymbol{\\psi} +  \\mathbf{b}  +  \\boldsymbol{\\epsilon}_{q}\n",
    "\\end{array}\n",
    "\\right.\n",
    "\\end{align}\n",
    "$$\n",
    "where $\\mathbf{F}( \\boldsymbol{\\psi})$ and $ \\boldsymbol{\\epsilon}_\\psi$ are the augmented nonlinear operator and aleatoric uncertainties, respectively; $ \\mathbf{M} = \\left[ \\mathbf{0}~\\big|~\\mathbb{I}_{N_q}\\right]$ is the linear measurement operator; and $ \\mathbf{0}_{N_\\alpha}$ and $ \\mathbf{0}_{N_q}$ are vectors of zeros (because the parameters are constant in time, and $ \\mathbf{q}$ is not integrated in time but it is only computed at the analysis step).  \n",
    "\n",
    "<br />\n",
    "\n",
    "## 2.3. Model bias in a stochastic ensemble framework <a class=\"anchor\" id=\"bias-stoch\"></a>\n",
    "The underlying assumption of ensemble methods is that the ensemble is Gaussian distributed, i.e,  $ \\boldsymbol{\\psi}_j\\sim\\mathcal{N}(\\bar{ \\boldsymbol{\\psi}},  \\mathbf{C}_{\\psi\\psi})$. Within an ensemble framework, we approximate the model bias  with the ensemble statistics  \n",
    "$$\n",
    "\\begin{align}\n",
    "     \\mathbf{b} &\\approx \\mathbf{d}^{\\dagger} -  \\mathbf{M}\\bar{ \\boldsymbol{\\psi}}, \n",
    "\\end{align}\n",
    "$$\n",
    "Hence, the innovation of the ensemble mean can be approximated as \n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\bar{ \\mathbf{i}} =  \\mathbf{d} -  \\mathbf{M}\\bar{ \\boldsymbol{\\psi}} \\approx  \\mathbf{b}.  \n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "\n",
    "___\n",
    "\n",
    "#### Exercise \n",
    "<em><font color=#7CB9E8> \n",
    "<emsp> How does the definition of the innovaition change is we did not assume unbiased observations? This is, $\\mathbb{E}(\\mathbf{d}) \\neq  \\mathbf{d}^{\\dagger}$.\n",
    "<br />\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e422e2e2b8fe56e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<a class=\"anchor\" id=\"evolution\"></a>\n",
    "## 2.4. How do we estimate the bias in time? \n",
    "\n",
    "The model bias is defined as  is the expected difference between the true observable and the model observable. Therefore, it is unknown a priori (colloquially, it is an <em>unknown unknown</em>).  The model bias may be a function of the physical state, the environment or even a function of time. \n",
    "\n",
    "\n",
    "Recent advances in machine learning for data-driven modelling allow us to develop surrogate models of dynamical systems using neural networks. This is, we can use a neural network to estimate the bias of the low-order numerical models. Specifically, we employ an echo state network (ESN) to infer the model bias.  ESNs are suitable for real-time data assimilation because \n",
    "1. they are recurrent neural networks, i.e., they are designed to learn temporal dynamics in data; \n",
    "2. they are based on reservoir computing, hence they are universal approximators;\n",
    "3. they are general nonlinear auto-regressive models; and \n",
    "4. training an ESN consists of solving a linear regression problem, which provides a global minimum without backpropagation.\n",
    "\n",
    "The architecture of the model bias estimation by ESN is illustrated below. The network can evolve in open-loop (left) when observations are available or in closed-loop (right), in which the ESN runs autonomously. \n",
    "\n",
    "\n",
    "<a><img src=\"https://magrilab.ae.ic.ac.uk/uploads/9/6/2/1/96210914//esn-web_1.png\"></a> \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbac9a9ea5a7426",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "\n",
    "<a class=\"anchor\" id=\"rEnKF\"></a>\n",
    "# 3. The bias-regularized ensemble Kalman filter \n",
    "\n",
    "The objective function in a bias-regularized data assimilation framework contains three norms\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{J}( \\boldsymbol{\\psi}_j) = &\\left\\| \\boldsymbol{\\psi}_j- \\boldsymbol{\\psi}_j^\\mathrm{f}\\right\\|^2_{ \\mathbf{C}^{\\mathrm{f}^{-1}}_{\\psi\\psi}} +\n",
    " \\left\\|{ \\mathbf{y}}_j- \\mathbf{d}_j\\right\\|^2_{\\mathbf{C}^{-1}_{dd}}+\\gamma\\left\\| \\mathbf{b}_j\\right\\|^2_{ \\mathbf{C}^{-1}_{bb}}, \\quad \\mathrm{for} \\quad j=0,\\dots,m-1\n",
    "\\end{align}\n",
    "$$\n",
    "where the superscript 'f' indicates 'forecast'; the operator $\\left\\|\\cdot\\right\\|^2_{ \\mathbf{C}^{-1}}$ is the $L_2$-norm weighted by the semi-positive definite matrix ${ \\mathbf{C}^{-1}}$; $\\gamma\\geq0$ is a user-defined bias regularization factor; and $ \\mathbf{b}_j$ is the model bias of each ensemble member. For simplicity, we define the bias in the ensemble mean, such that  $ \\mathbf{b}_j =  \\mathbf{b}$ for all $j$. From left to right, the norms on the right-hand-side of the cost fuction measure \n",
    "1. the spread of the ensemble prediction, \n",
    "2. the distance between the bias-corrected estimate and the observables, and \n",
    "3. the model bias norm.\n",
    "\n",
    "The analytical solution of the bias-regularized ensemble Kalman filter (r-EnKF), which globally minimizes the cost function with respect to $ \\boldsymbol{\\psi}_j$, is \n",
    "$$\n",
    "\\begin{align}\n",
    "     \\boldsymbol{\\psi}_j^\\mathrm{a} &= \n",
    "     \\boldsymbol{\\psi}_j^\\mathrm{f} + \n",
    "     \\mathbf{K}_\\mathrm{r} \\left[\\left(\\mathbb{I}+  \\mathbf{J}\\right)\\left( \\mathbf{d}_j -  \\mathbf{y}_j^\\mathrm{f}\\right) - \\gamma  \\mathbf{C}_{dd} \\mathbf{C}^{-1}_{bb} \\mathbf{J} \\mathbf{b}^\\mathrm{f}\\right], \\quad j=0,\\dots,m-1\n",
    "\\end{align}\n",
    "$$\n",
    "with\n",
    "$$\n",
    "\\begin{align}\n",
    " \\mathbf{K}_\\mathrm{r} =  \\mathbf{C}_{\\psi\\psi}^\\mathrm{f} \\mathbf{M}^\\mathrm{T}\\left[ \\mathbf{C}_{dd} + (\\mathbb{I}+  \\mathbf{J}) \\mathbf{M} \\mathbf{C}_{\\psi\\psi}^\\mathrm{f} \\mathbf{M}^\\mathrm{T}(\\mathbb{I}+  \\mathbf{J})^\\mathrm{T} + \\gamma  \\mathbf{C}_{dd} \\mathbf{C}^{-1}_{bb} \\mathbf{J} \\mathbf{M} \\mathbf{C}_{\\psi\\psi}^\\mathrm{f} \\mathbf{M}^\\mathrm{T}{ \\mathbf{J}}^\\mathrm{T}\\right]^{-1},\n",
    "\\end{align}\n",
    "$$\n",
    "where 'a'  stands for 'analysis', i.e, the optimal state of the assimilation; $ \\mathbf{K}_\\mathrm{r}$ is the regularized Kalman gain matrix; and $ \\mathbf{J} =  \\mathrm{d}  \\mathbf{b}/ \\mathrm{d} \\mathbf{M} \\boldsymbol{\\psi}$ is the Jacobian of the bias estimator. We prescribe  $ \\mathbf{C}_{dd} =  \\mathbf{C}_{bb}$ because the model  bias is defined in the observable space.  We use $\\gamma$ to tune the norm of the bias. The optimal state and parameters are\n",
    " $$\n",
    "\\begin{align}\n",
    "    \\begin{bmatrix}\n",
    "         \\boldsymbol{\\phi}_j^\\mathrm{a}\\\\\n",
    "         \\boldsymbol{\\alpha}_j^\\mathrm{a}\n",
    "    \\end{bmatrix}\n",
    "    = \n",
    "    \\begin{bmatrix}\n",
    "         \\boldsymbol{\\phi}_j^\\mathrm{f}\\\\\n",
    "         \\boldsymbol{\\alpha}_j^\\mathrm{f}\n",
    "    \\end{bmatrix} + \n",
    "    \\overbrace{\n",
    "    \\begin{bmatrix}\n",
    "         \\mathbf{C}_{\\phi q}^\\mathrm{f}\\\\\n",
    "         \\mathbf{C}_{\\alpha q}^\\mathrm{f}\n",
    "    \\end{bmatrix}\n",
    "        \\left\\{ \\mathbf{C}_{dd}+(\\mathbb{I}+  \\mathbf{J}) \\mathbf{C}_{qq}^\\mathrm{f}(\\mathbb{I}+  \\mathbf{J})^\\mathrm{T} +\\gamma  \\mathbf{J} \\mathbf{C}_{qq}^\\mathrm{f}{ \\mathbf{J}}^\\mathrm{T}\\right\\}^{-1}\n",
    "    }^\\mathrm{Regularized\\, Kalman \\, gain,\\,  \\mathbf{K}_\\mathrm{r}} \\Big[\\left(\\mathbb{I}+\\mathbf{J}\\right)\\left(\\mathbf{d}_j -  \\mathbf{y}_j^\\mathrm{f}\\right)- \\gamma  \\mathbf{J} \\mathbf{b}^\\mathrm{f}\\Big]\n",
    "\\end{align}\n",
    "$$\n",
    "The r-EnKF defines a `good' analysis from a biased model if  the unbiased state $ \\mathbf{y}$ matches the truth, and the model bias $ \\mathbf{b}$ is small relative to the truth. The underlying assumptions of this work are that (i) our low-order model is qualitatively accurate such that the model bias $ \\mathbf{b}^\\mathrm{f}$ has a small norm; and (ii) the sensors are properly calibrated.\n",
    "\n",
    "<br />\n",
    "\n",
    "___\n",
    "\n",
    "    \n",
    "#### Exercise\n",
    "<em><font color=#7CB9E8> Proof that in the limiting case when the assimilation framework is unbiased, the r-EnKF becomes the bias-unregularized EnKF, which we discussed in the previous tutorials.\n",
    "</font>\n",
    "<br />\n",
    "\n",
    "___\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "b8ff707804d7f85",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-06-04T17:50:19.382496Z",
     "start_time": "2024-06-04T17:50:19.377553Z"
    }
   },
   "source": [
    "import scipy.linalg as la \n",
    "def rBA_EnKF(Af, d, Cdd, Cbb, k, M, b, J):\n",
    "    \"\"\" Bias-aware Ensemble Kalman Filter.\n",
    "        Inputs:\n",
    "            Af: forecast ensemble at time t (augmented with Y) [N x m]\n",
    "            d: observation at time t [Nq x 1]\n",
    "            Cdd: observation error covariance matrix [Nq x Nq]\n",
    "            Cbb: bias covariance matrix [Nq x Nq]\n",
    "            k: bias penalisation factor\n",
    "            M: matrix mapping from state to observation space [Nq x N]\n",
    "            b: bias of the forecast observables (Y = MAf + B) [Nq x 1]\n",
    "            J: derivative of the bias with respect to the input [Nq x Nq]\n",
    "        Returns:\n",
    "            Aa: analysis ensemble (or Af is Aa is not real)\n",
    "    \"\"\"\n",
    "    m = np.size(Af, 1)\n",
    "    Nq = len(d)\n",
    "\n",
    "    Iq = np.eye(Nq)\n",
    "    # Mean and deviations of the ensemble\n",
    "    Psi_f = Af - np.mean(Af, 1, keepdims=True)\n",
    "    S = np.dot(M, Psi_f)\n",
    "    Q = np.dot(M, Af)\n",
    "\n",
    "    # Create an ensemble of observations\n",
    "    D = rng.multivariate_normal(d, Cdd, m).transpose()\n",
    "\n",
    "    # Correct the model estimate with the bias\n",
    "    Y = Q + b\n",
    "\n",
    "    Cqq = np.dot(S, S.T)  # covariance of observations M Psi_f Psi_f.T M.T\n",
    "    if np.array_equiv(Cdd, Cbb):\n",
    "        CdWb = Iq\n",
    "    else:\n",
    "        CdWb = np.dot(Cdd, la.inv(Cbb))\n",
    "\n",
    "    Cinv = ((m - 1) * Cdd + np.dot(Iq + J, np.dot(Cqq, (Iq + J).T)) + k * np.dot(CdWb, np.dot(J, np.dot(Cqq, J.T))))\n",
    "\n",
    "    K = np.dot(Psi_f, np.dot(S.T, la.inv(Cinv)))\n",
    "    Aa = Af + np.dot(K, np.dot(Iq + J, D - Y) - k * np.dot(CdWb, np.dot(J, np.repeat(b, m, axis=1))))\n",
    "\n",
    "    return Aa\n"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "e914bc441d611707",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-06-04T17:50:19.403011Z",
     "start_time": "2024-06-04T17:50:19.400025Z"
    }
   },
   "source": [
    "%matplotlib widget\n",
    "import numpy as np\n",
    "\n",
    "rng = np.random.default_rng(0)"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8ff8b43e-aab7-40e4-b795-a8faf3cdfe0d",
   "metadata": {},
   "source": [
    "<a id=\"test\"></a>\n",
    "## 3.1. Test case: van der Pol model with manually added bias "
   ]
  },
  {
   "cell_type": "code",
   "id": "5e16e73db6e66408",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-06-04T17:50:41.108195Z",
     "start_time": "2024-06-04T17:50:40.154594Z"
    }
   },
   "source": [
    "%matplotlib widget\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from essentials.physical_models import VdP\n",
    "from essentials.create import create_truth\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "dt_t = 2e-4\n",
    "\n",
    "\n",
    "# The manual bias is a function of state and/or time\n",
    "def manual_bias(y, t):\n",
    "    # Linear function of the state\n",
    "    return .2 * y + .3 * np.max(y, axis=0), 'linear'\n",
    "    # Periodic function of the state\n",
    "    # return 0.5 * np.max(y, axis=0) * np.cos(2 * y / np.max(y, axis=0)), 'periodic'\n",
    "    # Time-varying bias\n",
    "    # return .4 * y * np.sin((np.expand_dims(t, -1) * np.pi * 2) ** 2), 'time'\n",
    "\n",
    "\n",
    "true_params = dict(model=VdP,\n",
    "                   t_start=1.5,\n",
    "                   t_stop=1.8,\n",
    "                   t_max=2.5,\n",
    "                   Nt_obs=30,\n",
    "                   dt=dt_t,\n",
    "                   psi0=rng.random(2)+5,\n",
    "                   std_obs=0.1,\n",
    "                   noise_type='gauss,additive',\n",
    "                   manual_bias=manual_bias\n",
    "                   )\n",
    "\n",
    "truth = create_truth(**true_params)\n",
    "y_obs, t_obs = [truth[key].copy() for key in ['y_obs', 't_obs']]\n",
    "\n"
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "c7731822-3367-471c-9117-3f6ff58674d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T17:50:44.649868Z",
     "start_time": "2024-06-04T17:50:44.028961Z"
    }
   },
   "source": [
    "# Visualize the truth and observations\n",
    "from essentials.plotResults import plot_truth\n",
    "plot_truth(f_max=300, window=0.1, **truth)"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "76ee7c8e-4dfa-428c-98e0-e27e69fe10fb",
   "metadata": {},
   "source": [
    "### ii. Define ensemble"
   ]
  },
  {
   "cell_type": "code",
   "id": "866c1f52-ec31-4dd1-beaf-ddae1749357f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T17:50:51.015871Z",
     "start_time": "2024-06-04T17:50:51.006869Z"
    }
   },
   "source": [
    "from essentials.create import create_ensemble\n",
    "\n",
    "forecast_params = dict(filter='rBA_EnKF',\n",
    "                       m=10,\n",
    "                       dt=dt_t,\n",
    "                       model=VdP,\n",
    "                       est_a=dict(zeta=(40, 80.),\n",
    "                                 beta=(50, 80),\n",
    "                                 kappa=(3, 5),\n",
    "                                 ),\n",
    "                       std_psi=0.3,\n",
    "                       alpha_distr='uniform',\n",
    "                       inflation=1.0,\n",
    "                       regularization_factor=0.\n",
    "                       )\n",
    "\n",
    "\n",
    "y_obs, t_obs = [truth[key].copy() for key in ['y_obs', 't_obs']]\n",
    "\n",
    "ensemble = create_ensemble(**forecast_params)"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "41099a40-964c-4d94-9819-b4d8f9f42367",
   "metadata": {},
   "source": [
    "### iii. Train an ESN to model the model bias\n",
    "The procedure is the following:\n",
    "- Initialise ESN Bias class object\n",
    "- Create synthetic bias to use as training data \n",
    "- Train the ESN\n",
    "- Create washout data\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "85f1f360-e17e-4d26-8d2f-e6cfb5ab1d8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T17:51:45.081054Z",
     "start_time": "2024-06-04T17:51:00.302942Z"
    }
   },
   "source": [
    "from essentials.create import create_bias_model\n",
    "from essentials.bias_models import ESN\n",
    "\n",
    "\n",
    "train_params = dict(bias_model=ESN, \n",
    "                    upsample=2,\n",
    "                    N_units=40,\n",
    "                    N_wash=10,\n",
    "                    t_train=ensemble.t_CR * 5,\n",
    "                    t_test=ensemble.t_CR * 1,\n",
    "                    t_val=ensemble.t_CR * 1,\n",
    "                    # Training data generation options\n",
    "                    augment_data=True,\n",
    "                    biased_observations=True,\n",
    "                    L=20,\n",
    "                    # Hyperparameter search ranges\n",
    "                    rho_range=(0.4, 1.),\n",
    "                    sigma_in_range=(np.log10(1e-5), np.log10(1e1)),\n",
    "                    tikh_range=[1e-16]\n",
    "                    )\n",
    "\n",
    "bias_model, wash_obs, wash_t = create_bias_model(ensemble, truth.copy(), bias_params=train_params)\n",
    "\n",
    "\n",
    "\n",
    "ensemble.bias = bias_model.copy()\n"
   ],
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "6dd91d5c-6498-4175-b62d-ea38a84badd5",
   "metadata": {},
   "source": [
    "### iv. Run simulaiton"
   ]
  },
  {
   "cell_type": "code",
   "id": "45eb9680-6cf9-4885-bff8-e99aad931167",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T17:51:50.723019Z",
     "start_time": "2024-06-04T17:51:45.082746Z"
    }
   },
   "source": [
    "from essentials.DA import dataAssimilation\n",
    "\n",
    "ensemble.regularization_factor = 5.\n",
    "filter_ens = dataAssimilation(ensemble.copy(), y_obs=y_obs, t_obs=t_obs, std_obs=0.01, wash_t=wash_t, wash_obs=wash_obs)"
   ],
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "e18262a1-9099-4cd2-8bc6-b5df365c61fe",
   "metadata": {},
   "source": [
    "### v. Plot results"
   ]
  },
  {
   "cell_type": "code",
   "id": "578667fd-5ccf-4f67-906e-1de62b44b254",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T17:51:54.381704Z",
     "start_time": "2024-06-04T17:51:50.724315Z"
    }
   },
   "source": [
    "from essentials.plotResults import plot_timeseries, plot_parameters\n",
    "\n",
    "# Visualize attractors\n",
    "case0 = truth['case'].copy()\n",
    "case1 = filter_ens.copy()\n",
    "\n",
    "# Forecast the ensemble further without assimilation\n",
    "ens = filter_ens.copy()\n",
    "\n",
    "Nt = int(4 * true_params['Nt_obs'])\n",
    "psi, t = ens.time_integrate(Nt, averaged=False)\n",
    "ens.update_history(psi, t)\n",
    "\n",
    "y = ens.get_observable_hist(Nt)\n",
    "b, t_b = ens.bias.time_integrate(t=t, y=y)\n",
    "ens.bias.update_history(b, t_b)\n",
    "\n",
    "\n",
    "plot_timeseries(ens, truth.copy(), plot_ensemble_members=False, plot_bias=True)\n",
    "plot_parameters(ens, truth.copy(), reference_p=VdP.defaults)\n"
   ],
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "6e1fc76d-0ae3-4c2d-8ed8-7aa3c5f40692",
   "metadata": {},
   "source": [
    "\n",
    "___\n",
    "\n",
    "#### Exercise \n",
    "<em><font color=#7CB9E8> \n",
    "<emsp> How does the solution change as we vary the  bias regularization factor, $\\gamma$ ? What happens if $\\gamma=0$? And if  $\\gamma=25$? \n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "id": "00040bda-0c04-45b8-98d4-ac47d5c3a6e7",
   "metadata": {},
   "source": [],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
